{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ac5fc1",
   "metadata": {},
   "source": [
    "Creation of output files for further analysis\n",
    "\n",
    "requirements:\n",
    "\n",
    "pandas == 1.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db20348",
   "metadata": {
    "id": "1db20348"
   },
   "source": [
    "# | Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "# spaCy - Named Entity, Vectorization\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# QOL\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Language Detection\n",
    "\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d259d0",
   "metadata": {
    "id": "e0d259d0",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/Users/landsiedelj/Downloads/export_no_duplicates.pickle')\n",
    "df_person = pd.read_excel('/Users/landsiedelj/Downloads/WikiPersonen.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ee049",
   "metadata": {
    "id": "4d4ee049"
   },
   "source": [
    "# | Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d74e8f",
   "metadata": {
    "id": "b0d74e8f",
    "outputId": "474ae5f8-b700-4369-d3fb-d3d55d308a64",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df_person.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.search_engine.value_counts())\n",
    "print(df.depth.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4227c82",
   "metadata": {
    "id": "a4227c82",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning df_person\n",
    "wiki_dictionary = {\n",
    "                \"Wikipedia-Eintrag\": \"wikipedia_eintrag\",\n",
    "                \"Name, Vorname\": \"name_vorname\",\n",
    "                \"alle Berufe\": \"alle_berufe\",\n",
    "                \"1. Beruf\": \"1_beruf\",\n",
    "                \"Beruf - Oberkategorie\": \"beruf_oberkategorie\",\n",
    "                \"Seitenaufrufe Wikipedia der letzten 12 Monate\": \"seitenaufrufe\"}\n",
    "    \n",
    "df_person = df_person.copy()\n",
    "df_person.rename(columns = wiki_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572747f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0972f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51cf5c",
   "metadata": {
    "id": "be51cf5c",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Unify terms\n",
    "def clean_target_add(x):\n",
    "    return str(x).replace(\"cv\",\"lebenslauf\").replace(\"vs\",\"versus\").replace(\"bvb\",\"borussia dortmund\").replace(\"cpr\",\"herz-lungen-wiederbelebung\") \\\n",
    "        .replace(\"chancellor\",\"bundeskanzler\").replace(\"acab\",\"alle polizisten sind bastarde\").replace(\"city\",\"stadt\").replace(\"age\",\"alter\") \\\n",
    "            .replace(\"contemporary\", \"zeitgemäß\").replace(\"concert\", \"konzert\").replace(\"collection\", \"kollection\").replace(\"chords\", \"akkorde\") \\\n",
    "                .replace(\"birth chart\", \"geburtshoroskop\").replace(\"challenge\", \"herausforderung\").replace(\"child\", \"kind\").replace(\"analysis\", \"analyse\") \\\n",
    "                    .replace(\"book\", \"buch\").replace(\"car\", \"auto\").replace(\"cast\", \"besetzung\").replace(\"closet\", \"kleiderschrank\")\n",
    "        \n",
    "\n",
    "\n",
    "df['target_add'] = df['target_add'].parallel_apply(clean_target_add)\n",
    "\n",
    "df['root'] = df['root'].apply(lambda x: re.sub(\"bastian yottta\",\"bastian yotta\", str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf100f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wiki_eintrag(x):\n",
    "    return str(x).replace(\"Jérôme Boateng\",\"Jerome Boateng\")\n",
    "\n",
    "df_person['wikipedia_eintrag'] = df_person['wikipedia_eintrag'].apply(clean_wiki_eintrag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f54672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning \"Beruf_Oberkategorie\" for better readability\n",
    "\n",
    "di = {\n",
    "\"ADE\":\"Adel\",\n",
    "\"SEC\" :\"Sicherheitsdienste\",\n",
    "\"EKM\" :\t\"Entertainment\",\n",
    "\"POL\" :\t\"Politik\",\n",
    "\"REL\" :\t\"Religion\",\n",
    "\"SPO\" :\t\"Sport\",\n",
    "\"CRI\" :\t\"Verbrechen\",\n",
    "\"ECO\" :\t\"Wirtschaft\",\n",
    "\"WIS\" :\t\"Wissen\"}\n",
    "\n",
    "df_person[\"beruf_oberkategorie\"].replace(di, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop not needed col\n",
    "df.drop(['datetime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a099d2e",
   "metadata": {
    "id": "0a099d2e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Strip whitespace\n",
    "df['target_add'] = df['target_add'].str.split(',').str[-1].str.lstrip() # del whitespace\n",
    "df['source_add'] = df['source_add'].str.split(',').str[-1].str.lstrip() # del whitespace\n",
    "df['root'] = df['root'].str.split(',').str[-1].str.lstrip() # del whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8767fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace\n",
    "df_person['wikipedia_eintrag'] = df_person['wikipedia_eintrag'].str.split(',').str[-1].str.lstrip() # del whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700c8ed",
   "metadata": {},
   "source": [
    "### If root in excel add gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person['wikipedia_eintrag'] = df_person['wikipedia_eintrag'].str.replace(r'\\(.*\\)', '') # del everything in parenthesis\n",
    "df_person = df_person.apply(lambda x: x.astype(str).str.lower())\n",
    "df_person['wikipedia_eintrag'] = df_person['wikipedia_eintrag'].str.rstrip() # del whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f589e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map gender to Entity, if in Wiki data set -> add new column\n",
    "df['gender'] = df.UID.map(df_person.set_index('UID')['Geschlecht'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.gender.value_counts())\n",
    "print(df.gender.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a545548",
   "metadata": {},
   "source": [
    "### If root in excel add occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12caa35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['occupation'] = df.UID.map(df_person.set_index('UID')['beruf_oberkategorie'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(target_add=df['target_add'].str.split(' ')).explode('target_add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107eaf3",
   "metadata": {
    "id": "b107eaf3"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa64e6",
   "metadata": {
    "id": "8eaa64e6",
    "outputId": "a37f84c9-e845-4978-e1b5-6b03354e81cd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# With RegexpTokenizer nltk module ->  take only tokens from words and numbers\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df[\"tokens\"] = df.progress_apply(lambda row: tokenizer.tokenize(str(row[\"target_add\"].lower())), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f649e96",
   "metadata": {
    "id": "2f649e96"
   },
   "source": [
    "# | Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035bc52",
   "metadata": {
    "id": "4035bc52",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH, Language.TURKISH, Language.DUTCH, Language.ITALIAN, Language.POLISH, Language.RUSSIAN]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "df['language'] = df['target'].parallel_apply(lambda x:detector.detect_language_of(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ea8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab800cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['language'] == Language.GERMAN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980339d",
   "metadata": {
    "id": "b980339d"
   },
   "source": [
    "# | Linguistic Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbbf91",
   "metadata": {
    "id": "30cbbf91"
   },
   "source": [
    "## | Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51066db",
   "metadata": {
    "id": "e51066db",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"lemmata\"] = df.apply(lambda row: [lemmatizer.lemmatize(word) for word in row[\"tokens\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bec8bd",
   "metadata": {
    "id": "38bec8bd",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatization with word type from https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "df[\"lemmata_word_type\"] = df.parallel_apply(lambda row: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in row[\"tokens\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a3f1c",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98407d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of target_add -> suggestion terms\n",
    "\n",
    "suggestion_list = df['target_add'].drop_duplicates().tolist()\n",
    "print(len(suggestion_list))\n",
    "\n",
    "suggestion_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open('/Users/landsiedelj/Downloads/suggestion_list.txt', 'w', encoding='utf-8')\n",
    "for element in suggestion_list:\n",
    "    textfile.write(element + '\\n')\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('export_incl_gender_cleaned_occupation_NEU.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ffcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('export_incl_gender_cleaned_occupation_NEU.pkl')\n",
    "\n",
    "# Datei ist hier abgelegt: https://th-koeln.sciebo.de/s/M3q60FEe0i5bduT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db305f8c",
   "metadata": {
    "id": "db305f8c"
   },
   "source": [
    "# | Vectorisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4bc4f",
   "metadata": {
    "id": "53e4bc4f"
   },
   "source": [
    "## | SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68045307",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Testing Vectorization with a subset\n",
    "df_2 = df.copy()[:5000]\n",
    "token_list = df_2.tokens\n",
    "# Creating Single Terms\n",
    "token_list = [a for b in token_list for a in b]\n",
    "token_list = token_list[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9309ae",
   "metadata": {
    "id": "ec9309ae",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating Vector array for our subset\n",
    "doc = list(nlp.pipe(token_list, disable=['parser', 'tagger', 'ner']))\n",
    "vectors = [term.vector for term in doc]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DIS22_bias_query_pipeline_030622.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "3d1c3d4a7f8058a677e0e869aeedd37e6b716847110188d754d93c56bb0e75db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
