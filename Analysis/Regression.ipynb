{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#|-Preliminaries\" data-toc-modified-id=\"|-Preliminaries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>| Preliminaries</a></span></li><li><span><a href=\"#|-Cleaning\" data-toc-modified-id=\"|-Cleaning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>| Cleaning</a></span></li><li><span><a href=\"#|-Tokenization\" data-toc-modified-id=\"|-Tokenization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>| Tokenization</a></span></li><li><span><a href=\"#|-POS-Tagging\" data-toc-modified-id=\"|-POS-Tagging-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>| POS Tagging</a></span></li><li><span><a href=\"#|-Language-Detection\" data-toc-modified-id=\"|-Language-Detection-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>| Language Detection</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Ratio-other-languages\" data-toc-modified-id=\"Ratio-other-languages-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Ratio other languages</a></span></li></ul></li></ul></li><li><span><a href=\"#|-Linguistic-Processing\" data-toc-modified-id=\"|-Linguistic-Processing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>| Linguistic Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#|-Lemmatization\" data-toc-modified-id=\"|-Lemmatization-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>| Lemmatization</a></span></li></ul></li><li><span><a href=\"#|-Frequencies-for-categories\" data-toc-modified-id=\"|-Frequencies-for-categories-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>| Frequencies for categories</a></span></li><li><span><a href=\"#|-Vectorisierung\" data-toc-modified-id=\"|-Vectorisierung-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>| Vectorisierung</a></span><ul class=\"toc-item\"><li><span><a href=\"#|-SpaCy\" data-toc-modified-id=\"|-SpaCy-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>| SpaCy</a></span></li><li><span><a href=\"#|-Gensim\" data-toc-modified-id=\"|-Gensim-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>| Gensim</a></span></li></ul></li></ul></div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# | Preliminaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#! pip install spacy==3.0.0\n",
    "#! python -m spacy download de_core_news_lg\n",
    "#! pip3 install https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.0.0/de_core_news_lg-3.0.0-py3-none-any.whl\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Regex for Cleaning\n",
    "import regex as re\n",
    "\n",
    "# spaCy - Named Entity, Vecotorization\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# QOL\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Gensim further testing Vectorization methods\n",
    "import gensim.downloader as dl\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Scikit learn for testing Clusteringmethods\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Language Detection\n",
    "# import langdetect\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r'/Users/anna/PycharmProjects/NLP/export.pickle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# | Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(['search_engine','edge','datetime'], axis=1, inplace=True)\n",
    "df[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dictionary = {\"Unnamed: 0\" : \"id\"}\n",
    "    \n",
    "df = df.copy()\n",
    "df.rename(columns=dictionary, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Unify terms\n",
    "df['target_add'] = df['target_add'].apply(lambda x: re.sub(\"cv\",\"lebenslauf\", str(x)))\n",
    "df['target_add'] = df['target_add'].apply(lambda x: re.sub(\"vs\",\"versus\", str(x)))\n",
    "df['target_add'] = df['target_add'].apply(lambda x: re.sub(\"bvb\",\"borussia dortmund\", str(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Strip whitespace\n",
    "df['target_add'] = df['target_add'].str.split(',').str[-1].str.lstrip() # del whitespace\n",
    "df['source_add'] = df['source_add'].str.split(',').str[-1].str.lstrip() # del whitespace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# | Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# With RegexpTokenizer nltk module ->  take only tokens from words and numbers\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df[\"tokens\"] = df.progress_apply(lambda row: tokenizer.tokenize(str(row[\"target_add\"].lower())), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# | POS Tagging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determine POS tags for tokens\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "df[\"pos\"] = df.progress_apply(lambda row: nltk.pos_tag(row[\"tokens\"]), axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entity Recognition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "##Enitity Recognition von Städten und Personen:\n",
    "#https://spacy.io/usage/linguistic-features\n",
    "\n",
    "#! pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.0.0/de_core_news_sm-3.0.0-py3-none-any.whl\n",
    "#! pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.0.0/de_core_news_sm-3.0.0.tar.gz\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "df['person'] = ''\n",
    "df['location'] = ''\n",
    "df['organization'] = ''\n",
    "df['misc'] = ''\n",
    "for row in df.itertuples(index=True, name='Pandas'):\n",
    "    doc = nlp(str(row.target_add))\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'LOC':\n",
    "            df.at[row.Index, 'location'] = ent.text\n",
    "        elif ent.label_ == 'MISC':\n",
    "            df.at[row.Index, 'misc'] = ent.text\n",
    "        elif ent.label_ == 'PER':\n",
    "            df.at[row.Index, 'person'] = ent.text\n",
    "        elif ent.label_ == 'ORG':\n",
    "            df.at[row.Index, 'organization'] = ent.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['location']\n",
    "df.to_csv('out.csv', index=False, sep=';')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option('display.min_rows', 150)\n",
    "pd.options.display.max_colwidth = 10000\n",
    "print(df[['person', 'location', 'organization', 'misc']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# | Linguistic Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## | Lemmatization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"lemmata\"] = df.apply(lambda row: [lemmatizer.lemmatize(word) for word in row[\"tokens\"]], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lemmatization with word type from https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "df[\"lemmata_word_type\"] = df.apply(lambda row: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in row[\"tokens\"]], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# | x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['target_add']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list1 = ['common terms','consulting gmbh','aktuelle teams','breaking news','eis cream','biontech impfung', 'aktueller verein', 'collapse full video', 'brandon williams', 'corona']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bi-grams "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.0.0/de_core_news_sm-3.0.0-py3-none-any.whl\n",
    "#! pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.0.0/de_core_news_sm-3.0.0.tar.gz\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "\n",
    "#import pyLDAvis\n",
    "\n",
    "#import pyLDAvis.gensim_models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "import nltk \n",
    "#nlp = spacy.load(\"de_core_news_sm-3.0.0\")\n",
    "nlp = spacy.load('de_core_news_sm',disable=['parser', 'ner'])\n",
    "\n",
    "#importing the Stopwords to use them\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('german')\n",
    "\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','for'])\n",
    "\n",
    "#downloading the data\n",
    "data = df['target_add']\n",
    "\n",
    "processed_data = list(tokeniz(data))\n",
    "\n",
    "#Building Bigram & Trigram Models\n",
    "\n",
    "bigram = gensim.models.Phrases(processed_data, min_count=5, threshold=100)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[processed_data], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "#function to filter out stopwords\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "#function to create bigrams\n",
    "\n",
    "def create_bigrams(texts):\n",
    "\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "#function to create trigrams\n",
    "\n",
    "def create_trigrams(texts):\n",
    "\n",
    "    [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "#function for lemmatization\n",
    "\n",
    "def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "\n",
    "    texts_op = []\n",
    "\n",
    "    for sent in texts:\n",
    "\n",
    "        doc = nlp(\" \".join(sent))\n",
    "\n",
    "        texts_op.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\n",
    "    return texts_op\n",
    "\n",
    "#removing stopwords, creating bigrams and lemmatizing the text\n",
    "\n",
    "data_wo_stopwords = remove_stopwords(processed_data)\n",
    "\n",
    "data_bigrams = create_bigrams(data_wo_stopwords)\n",
    "\n",
    "data_lemmatized = lemmatize(data_bigrams, allowed_postags=[ 'NOUN', 'ADJ', 'VERB'])\n",
    "\n",
    "#printing the lemmatized data\n",
    "\n",
    "print(data_lemmatized[:3])\n",
    "\n",
    "#creating a dictionary\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "texts = data_lemmatized\n",
    "\n",
    "#building a corpus for the topic model\n",
    "\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#printing the corpus we created above.\n",
    "\n",
    "print(gensim_corpus[:3]) \n",
    "\n",
    "#we can print the words with their frequencies.\n",
    "\n",
    "[[(gensim_dictionary[id], freq) for id, freq in cp] for cp in gensim_corpus[:4]] \n",
    "\n",
    "#creating the LSI model \n",
    "\n",
    "lsi_model = gensim.models.lsimodel.LsiModel(\n",
    "\n",
    "   corpus=gensim_corpus, id2word=gensim_dictionary, num_topics=20,chunksize=100\n",
    "\n",
    ")\n",
    "\n",
    "#viewing topics\n",
    "\n",
    "pprint(lsi_model.print_topics())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# | Vectorisierung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## | SpaCy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\n",
    "# https://stackoverflow.com/questions/62624284/how-to-save-word-vectors-in-spacy\n",
    "# Testing Vectorization with a subset\n",
    "df_2 = df.copy()[:5000]\n",
    "token_list = df_2.tokens\n",
    "# Creating Single Terms\n",
    "token_list = [a for b in token_list for a in b]\n",
    "token_list = token_list[:5000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating Vector array for our subset\n",
    "doc = list(nlp.pipe(token_list, disable=['parser', 'tagger', 'ner']))\n",
    "vectors = [term.vector for term in doc]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "F: Enthalten von verschiedenen Suchmaschinen vorgeschlagene Search Query Suggestions einen Bias bei Personensuchen, z. B. im Hinblick auf Geschlecht, Berufsgruppe und Herkunft? Eine Untersuchung mithilfe von Recursive Algorithm Interrogation (RAI) und Alphabeterweiterung."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Einfache lineare Regression."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Anhand der Grafik kann deskriptiv bestimmt werden wie der Verlauf ist (linear, quadratisch, kubisch, loess )\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "def f(n,t):\n",
    "    x = [1,2,3,4,5,6,7,8] #hier kommen die UV rein (Anzahl der jeweiligen Gruppe (Sporter,Musiker, weitere Berufungen)\n",
    "    #Die Gruppe beeinflusst die Begriffe\n",
    "    y = [100,90,80,60,60,55,60,50] #hier kommen die A-Variablen rein (Anzahl der Begriffe eines Themengebiets (z.b. Familie))\n",
    "# z = ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C']\n",
    "    grün = plt.scatter(x, y, c='green')\n",
    "    \n",
    "    x = [5,6,7,4,5,6,7,8] #hier kommen die UA-Variable rein (Anzahl der jeweiligen Gruppe)\n",
    "    y = [99,90,80,60,60,55,60,50] \n",
    "    rot = plt.scatter(x, y, c='red')\n",
    "\n",
    "    curve_model = numpy.poly1d(numpy.polyfit(x, y, n))\n",
    "\n",
    "    curve_fitting_line = numpy.linspace(1, t, 100)\n",
    "    plt.plot(curve_fitting_line, curve_model(curve_fitting_line))\n",
    "    plt.legend((grün, rot), ('Sportler', 'Musiker'), scatterpoints=1,\n",
    "           loc='lower left',\n",
    "           ncol=3,\n",
    "           fontsize=9)\n",
    "    plt.title('Thema: Begriffe zur Familie von weiblicher Personen in Zusammenhang zu den Berufen')\n",
    "    plt.ylabel(\"Frequencies (Familie)\")\n",
    "    plt.xlabel(\"Anzahl Berufe (gruppiert)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interactive_plot= interactive(f,n=(1,20),t = (1,30))\n",
    "interactive_plot\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Quelle: https://statologie.de/lineare-regression-python/\n",
    "# Daten erstellen\n",
    "df3 = pd.DataFrame({'Anzahl der Gruppen (Sportler, Musiker usw.)': [1, 2, 2, 4, 2, 1, 5, 4, 2, 4, 4, 3, 6, 5, 3, 4, 6, 2, 1, 2],\n",
    "                   'Weiblich': [1, 3, 3, 5, 2, 2, 1, 1, 0, 3, 4, 3, 2, 4, 4, 4, 5, 1, 0, 1],\n",
    "                   'Anzahl der Begriffe (Familie)': [76, 78, 85, 88, 72, 69, 94, 94, 88, 92, 90, 75, 96, 90, 82, 85, 99, 83, 62, 76]})\n",
    "\n",
    "# Daten anzeigen \n",
    "df3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#python -m pip install statsmodels \n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Antwortvariable definieren\n",
    "y = df3['Anzahl der Begriffe (Familie)']\n",
    "\n",
    "# Prädiktorvariablen definieren\n",
    "x = df3[['Weiblich','Anzahl der Gruppen (Sportler, Musiker usw.)']]\n",
    "\n",
    "# Konstante zu Prädiktorvariablen hinzufügen\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# lineares Regressionsmodell anpassen\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "# Modellzusammenfassung anzeigen\n",
    "print(model.summary())\n",
    "\n",
    "#R-squared:\n",
    "#F-statistic prüft, ob das Modell insgesamt einen Erklärungsbeitrag leistet.\n",
    "#t: t-wert: \n",
    "#p: p-wert:\n",
    "#Die Signifikanz (p-Wert) sollte einen möglichst kleinen Wert (<0,05) haben. Wenn dem so ist, leistet das Regressionsmodell einen Erklärungsbeitrag."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beispiel Hypothese: Es gibt einen Zusammenhang zwischen des Themas Familie und weiblichen Sportlern oder Musikern.\n",
    "\n",
    "0,0 < 0,1\tkein Zusammenhang\n",
    "0,1 < 0,3\tgeringer Zusammenhang\n",
    "0,3 < 0,5\tmittlerer Zusammenhang\n",
    "0,5 < 0,7\thoher Zusammenhang\n",
    "0,7 < 1\tsehr hoher Zusammenhang\n",
    "\n",
    "R-squared:\n",
    "F-statistic prüft, ob das Modell insgesamt einen Erklärungsbeitrag leistet.\n",
    "t: t-wert: \n",
    "p: p-wert:\n",
    "Die Signifikanz (p-Wert) sollte einen möglichst kleinen Wert (<0,05) haben. Wenn dem so ist, leistet das Regressionsmodell einen Erklärungsbeitrag.\n",
    "Ist die Signifikanz über 0,05, leistet das Regressionsmodell keinen signifikanten Erklärungsbeitrag und das Verfahren bzw. die weitere Interpretation ist abzubrechen\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Der Koeffizient der unabhängigen Variable (Anzahl der Gruppe) wird wie folgt interpretiert: Wenn (Gruppe) um eine Einheit steigt (ein zusätzlicher Wert hinzukommt), so nimmt die Anzahl des bestimmten Themengebiets zu. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multiple lineare Regression.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vorraussetzung: Zwischen UA und AV besteht ein linearer Zusammenhang.\n",
    "Die multiple Regression ist wie die lineare Regression, aber mit mehr als einem UV, was bedeutet, dass wir versuchen, einen Wert basierend auf zwei oder mehr Variablen vorherzusagen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "X = df3[['Weiblich', 'Anzahl der Gruppen (Sportler, Musiker usw.)']] #UV\n",
    "y = df3['Anzahl der Begriffe (Familie)'] #AV\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "\n",
    "\n",
    "#Vorhersage: Anzahl der Begriffe von der Familie wo das weibliche Geschlecht 50x vorkommt und die Anzahl der Gruppen ist 1300x.\n",
    "predict = regr.predict([[200, 400]])\n",
    "\n",
    "print(predict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wir haben vorausgesagt, dass das weibliche Geschlecht, welches 200x in den Berufsbezeichnungen vorkommt etwa 2169x den Begriff Familie in den Suggestion enthält."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistische Regression.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beispiel: Vorhersage des Geschlechts einer Berufsgruppe zwischen 3 verschiedenen Themen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multivariate Regression."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Berechnung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LDA mit pyLDavis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Quelle: https://nbviewer.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb#topic=0&lambda=1&term=\n",
    "#Quelle: https://github.com/bmabey/pyLDAvis/issues/4\n",
    "#!pip install pyLDAvis\n",
    "from imp import reload\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import sys\n",
    "\n",
    "\n",
    "documents = df['tokens']\n",
    "\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, passes=10)\n",
    "\n",
    "vis_data = gensimvis.prepare(lda, corpus, dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vis_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visulaisierungen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "x = np.arange(5) \n",
    "y1 = [34, 56, 12, 89, 21] \n",
    "y2 = [12, 56, 78, 45, 22] \n",
    "y3 = [14, 23, 45, 25, 24] \n",
    "width = 0.2\n",
    "  \n",
    "plt.bar(x-0.2, y1, width, color='cyan') \n",
    "plt.bar(x, y2, width, color='orange') \n",
    "plt.bar(x+0.2, y3, width, color='green') \n",
    "plt.xticks(x, ['Sportler', 'Musiker', 'Influencer', 'Politiker', 'usw.']) \n",
    "plt.xlabel(\"Berufsgruppen\") \n",
    "plt.ylabel(\"Scores\") \n",
    "plt.legend([\"Familie\", \"Politik\", \"Technik\"]) \n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "DIS22_bias_query_pipeline_030622.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}